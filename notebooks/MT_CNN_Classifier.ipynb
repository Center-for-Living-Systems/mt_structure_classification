{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ldin/anaconda3/envs/pytorch_cuda124/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/ldin/anaconda3/envs/pytorch_cuda124/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ConvNeXt_Tiny_Weights.IMAGENET1K_V1`. You can also use `weights=ConvNeXt_Tiny_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 1.4339, Train Acc = 0.3191, Val Loss = 1.2435, Val Acc = 0.4930\n",
      "Epoch 2: Train Loss = 1.0348, Train Acc = 0.5638, Val Loss = 1.2071, Val Acc = 0.5352\n",
      "Epoch 3: Train Loss = 0.9820, Train Acc = 0.5922, Val Loss = 1.2539, Val Acc = 0.3944\n",
      "Epoch 4: Train Loss = 0.8200, Train Acc = 0.6915, Val Loss = 0.9965, Val Acc = 0.5634\n",
      "Epoch 5: Train Loss = 0.6368, Train Acc = 0.7482, Val Loss = 1.0345, Val Acc = 0.6056\n",
      "Epoch 6: Train Loss = 0.5026, Train Acc = 0.8298, Val Loss = 1.3209, Val Acc = 0.5493\n",
      "Epoch 7: Train Loss = 0.4328, Train Acc = 0.8333, Val Loss = 1.1639, Val Acc = 0.6197\n",
      "Epoch 8: Train Loss = 0.2395, Train Acc = 0.9326, Val Loss = 1.2754, Val Acc = 0.6479\n",
      "Epoch 9: Train Loss = 0.2202, Train Acc = 0.9220, Val Loss = 1.7112, Val Acc = 0.6056\n",
      "Epoch 10: Train Loss = 0.2343, Train Acc = 0.9113, Val Loss = 1.6136, Val Acc = 0.6056\n",
      "Epoch 11: Train Loss = 0.1195, Train Acc = 0.9645, Val Loss = 1.7290, Val Acc = 0.5915\n",
      "Epoch 12: Train Loss = 0.0613, Train Acc = 0.9858, Val Loss = 1.5944, Val Acc = 0.6197\n",
      "Epoch 13: Train Loss = 0.0329, Train Acc = 0.9965, Val Loss = 1.6143, Val Acc = 0.6056\n",
      "Epoch 14: Train Loss = 0.0184, Train Acc = 1.0000, Val Loss = 1.7448, Val Acc = 0.5775\n",
      "Epoch 15: Train Loss = 0.0161, Train Acc = 1.0000, Val Loss = 1.8540, Val Acc = 0.5775\n",
      "Epoch 16: Train Loss = 0.0176, Train Acc = 0.9965, Val Loss = 1.8573, Val Acc = 0.5915\n",
      "Epoch 17: Train Loss = 0.0135, Train Acc = 1.0000, Val Loss = 1.8524, Val Acc = 0.6056\n",
      "Epoch 18: Train Loss = 0.0086, Train Acc = 1.0000, Val Loss = 1.8322, Val Acc = 0.6056\n",
      "Early stopping triggered. Best Validation Accuracy: 0.647887323943662\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import pandas as pd\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load CSV\n",
    "data_csv = \"selected_for_annotation_mt_strcture_20250310_MA.csv\"\n",
    "image_dir = \"/mnt/d/lding/CLS/mousumiLiuDinner/set1to5_processed_results/Microtubule_GUV-Liu-20250106T211105Z-001/processed_MT/GUV-MT_obj_tiff_selected_std15/improved_png\"\n",
    "df = pd.read_csv(data_csv)\n",
    "df = df.dropna(subset=[\"filename\", \"label\"])\n",
    "df[\"label\"] = df[\"label\"].astype(str)\n",
    "\n",
    "# Split into Train (80%) & Validation (20%)\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, stratify=df[\"label\"], random_state=42)\n",
    "\n",
    "# Define Transforms\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "# Custom Dataset Class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe, root_dir, transform=None):\n",
    "        self.data = dataframe.copy()\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.label_to_index = {label: idx for idx, label in enumerate(sorted(self.data[\"label\"].unique()))}\n",
    "        self.data[\"label\"] = self.data[\"label\"].map(self.label_to_index)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.root_dir, self.data.iloc[idx, 0])\n",
    "        image = Image.open(img_name).convert(\"RGB\")\n",
    "        label = int(self.data.iloc[idx, 1])\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "# Create Datasets & DataLoaders\n",
    "train_dataset = CustomDataset(train_df, image_dir, transform=train_transform)\n",
    "val_dataset = CustomDataset(val_df, image_dir, transform=val_transform)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Function to initialize different models\n",
    "def initialize_model(model_name, num_classes):\n",
    "    if model_name == \"efficientnet\":\n",
    "        model = models.efficientnet_b0(pretrained=True)\n",
    "        num_ftrs = model.classifier[1].in_features\n",
    "        model.classifier[1] = nn.Linear(num_ftrs, num_classes)\n",
    "    elif model_name == \"resnet\":\n",
    "        model = models.resnet18(pretrained=True)\n",
    "        num_ftrs = model.fc.in_features\n",
    "        model.fc = nn.Linear(num_ftrs, num_classes)\n",
    "    elif model_name == \"convnext\":\n",
    "        model = models.convnext_tiny(pretrained=True)\n",
    "        num_ftrs = model.classifier[-1].in_features\n",
    "        model.classifier[-1] = nn.Linear(num_ftrs, num_classes)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported model type\")\n",
    "    return model.to(device)\n",
    "\n",
    "# Function to save confusion matrix and loss curves\n",
    "def save_plots(train_losses, val_losses, train_accuracies, val_accuracies, model_dir):\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.title('Loss Curve')\n",
    "    plt.savefig(f\"{model_dir}/loss_curve.png\")\n",
    "    plt.close()\n",
    "    \n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.plot(train_accuracies, label='Train Accuracy')\n",
    "    plt.plot(val_accuracies, label='Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.title('Accuracy Curve')\n",
    "    plt.savefig(f\"{model_dir}/accuracy_curve.png\")\n",
    "    plt.close()\n",
    "    \n",
    "# Function to plot confusion matrices\n",
    "def plot_confusion_matrix(model, dataloader, dataset_name, model_dir):\n",
    "    model.eval()\n",
    "    true_labels, pred_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "            pred_labels.extend(preds.cpu().numpy())\n",
    "    \n",
    "    cm = confusion_matrix(true_labels, pred_labels)\n",
    "    cm_percentage = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100  # Convert to percentages\n",
    "    labels = list(train_dataset.label_to_index.keys())\n",
    "    cm_df = pd.DataFrame(cm_percentage, index=labels, columns=labels)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm_df, annot=True, fmt=\".1f\", cmap=\"Blues\", linewidths=0.5)\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.title(f\"Confusion Matrix ({dataset_name}) - Percentage\")\n",
    "    plt.savefig(f\"{model_dir}/confusion_matrix_{dataset_name}_percent.png\")\n",
    "    plt.close()\n",
    "\n",
    "    cm_df = pd.DataFrame(cm, index=labels, columns=labels)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm_df, annot=True, fmt=\"d\", cmap=\"Blues\", linewidths=0.5)\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.title(f\"Confusion Matrix ({dataset_name})\")\n",
    "    plt.savefig(f\"{model_dir}/confusion_matrix_{dataset_name}_no.png\")\n",
    "    plt.close()\n",
    "\n",
    "# Run training function with added confusion matrix plotting\n",
    "def train_model(model_name, learning_rate=0.0003, weight_decay=1e-4, num_epochs=50, patience=10):\n",
    "    model = initialize_model(model_name, len(train_dataset.label_to_index))\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20, eta_min=1e-6)\n",
    "    patience_counter = 0\n",
    "\n",
    "    best_val_acc = 0\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    model_dir = f\"saved_models/{model_name}_lr{learning_rate}_wd{weight_decay}_{timestamp}\"\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    \n",
    "    train_losses, val_losses, train_accuracies, val_accuracies = [], [], [], []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss, correct_train, total_train = 0.0, 0, 0\n",
    "        \n",
    "        for images, labels in train_dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct_train += (preds == labels).sum().item()\n",
    "            total_train += labels.size(0)\n",
    "        \n",
    "        train_loss = running_loss / len(train_dataloader)\n",
    "        train_acc = correct_train / total_train\n",
    "        \n",
    "        # Validation Step\n",
    "        model.eval()\n",
    "        running_val_loss, correct_val, total_val = 0.0, 0, 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_dataloader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                running_val_loss += loss.item()\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                correct_val += (preds == labels).sum().item()\n",
    "                total_val += labels.size(0)\n",
    "        \n",
    "        val_loss = running_val_loss / len(val_dataloader)\n",
    "        val_acc = correct_val / total_val\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}: Train Loss = {train_loss:.4f}, Train Acc = {train_acc:.4f}, Val Loss = {val_loss:.4f}, Val Acc = {val_acc:.4f}\")\n",
    "    \n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        train_accuracies.append(train_acc)\n",
    "        val_accuracies.append(val_acc)\n",
    "        scheduler.step()\n",
    "\n",
    "        # Early Stopping\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), f\"{model_dir}/best_model.pth\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(\"Early stopping triggered. Best Validation Accuracy:\", best_val_acc)\n",
    "                break\n",
    "    \n",
    "    save_plots(train_losses, val_losses, train_accuracies, val_accuracies, model_dir)\n",
    "    plot_confusion_matrix(model, train_dataloader, \"Training Set\", model_dir)\n",
    "    plot_confusion_matrix(model, val_dataloader, \"Validation Set\", model_dir)\n",
    "    return model_dir\n",
    "\n",
    "# Run training\n",
    "model_dir = train_model(\"convnext\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_cuda124",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
